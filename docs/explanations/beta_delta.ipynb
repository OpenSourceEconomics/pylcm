{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-Delta (Quasi-Hyperbolic) Discounting\n",
    "\n",
    "This notebook shows how to implement quasi-hyperbolic discounting in PyLCM using a\n",
    "custom aggregation function $H$. It covers:\n",
    "\n",
    "1. The beta-delta discount function and its relation to PyLCM's $H$\n",
    "2. A simple 3-period consumption-savings model with analytical solutions\n",
    "3. Exponential, sophisticated, and naive agents\n",
    "4. Verifying numerical results against closed-form solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Beta-Delta Discounting\n",
    "\n",
    "Standard exponential discounting uses a single discount factor $\\delta$ to weight\n",
    "future utility:\n",
    "\n",
    "$$U_t = u_t + \\delta\\, u_{t+1} + \\delta^2\\, u_{t+2} + \\cdots$$\n",
    "\n",
    "Quasi-hyperbolic (beta-delta) discounting (Laibson, 1997) introduces a present-bias\n",
    "parameter $\\beta \\in (0, 1]$ that discounts *all* future periods relative to the\n",
    "present:\n",
    "\n",
    "$$U_t = u_t + \\beta\\delta\\, u_{t+1} + \\beta\\delta^2\\, u_{t+2} + \\cdots$$\n",
    "\n",
    "The discount weights are $\\{1,\\; \\beta\\delta,\\; \\beta\\delta^2,\\; \\ldots\\}$.\n",
    "When $\\beta = 1$ this reduces to exponential discounting. When $\\beta < 1$ the agent\n",
    "is present-biased — they overweight current utility relative to any future period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to PyLCM's $H$ Function\n",
    "\n",
    "PyLCM defines the value function recursively via an aggregation function $H$:\n",
    "\n",
    "$$V_t(s) = \\max_a \\; H\\bigl(u(s, a),\\; \\mathbb{E}_t[V_{t+1}(s')]\\bigr)$$\n",
    "\n",
    "The default $H$ is additive with a discount factor:\n",
    "$H(u, \\mathbb{E}[V']) = u + \\delta \\cdot \\mathbb{E}[V']$. For beta-delta discounting,\n",
    "we replace this with:\n",
    "\n",
    "$$H(u, \\mathbb{E}[V']) = u + \\beta\\delta \\cdot \\mathbb{E}[V']$$\n",
    "\n",
    "This works because the beta-delta discount function has the recursive structure:\n",
    "\n",
    "$$V_t = u_t + \\beta\\delta \\cdot V_{t+1}$$\n",
    "\n",
    "where each $V_{t+1}$ *already* includes the $\\beta$ factor for all periods beyond\n",
    "$t+1$. The $\\beta$ appears exactly once in each one-step discount — which is all $H$\n",
    "needs to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sophisticated vs. Naive Agents\n",
    "\n",
    "Beta-delta preferences are time-inconsistent: what the agent plans to do tomorrow\n",
    "differs from what they actually do when tomorrow arrives. This creates two behavioral\n",
    "types:\n",
    "\n",
    "- **Sophisticated agents** know their future selves will also be present-biased.\n",
    "  They correctly predict future behavior and optimize accordingly. The value function\n",
    "  $V$ is computed and used consistently with $\\beta\\delta$ discounting.\n",
    "\n",
    "- **Naive agents** believe their future selves will behave as exponential discounters\n",
    "  (with discount factor $\\delta$ only). They compute a perceived value function\n",
    "  $V^E$ using exponential discounting, but when choosing actions they apply present\n",
    "  bias $\\beta\\delta$ to the continuation value.\n",
    "\n",
    "In PyLCM terms:\n",
    "\n",
    "| Agent type | `solve` params | `simulate` params |\n",
    "|---|---|---|\n",
    "| Exponential | $\\beta=1, \\delta$ | same |\n",
    "| Sophisticated | $\\beta, \\delta$ | same |\n",
    "| Naive | $\\beta=1, \\delta$ (exponential $V^E$) | $\\beta, \\delta$ (present-biased choices) |\n",
    "\n",
    "The naive case needs different $H$ behavior in each phase. There are two ways to handle\n",
    "this:\n",
    "\n",
    "1. **Manual approach**: call `solve` and `simulate` separately with different params\n",
    "2. **`PhaseVariant`**: wrap $H$ in a `PhaseVariant` container that provides different\n",
    "   implementations for each phase, then use `solve_and_simulate` as usual\n",
    "\n",
    "Both approaches are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: A 3-Period Model\n",
    "\n",
    "We use a minimal consumption-savings model:\n",
    "\n",
    "- **3 periods**: $t = 0, 1$ (decisions), $t = 2$ (terminal)\n",
    "- **1 state**: wealth $w$\n",
    "- **1 action**: consumption $c$\n",
    "- **Utility**: $u(c) = \\log(c)$\n",
    "- **Budget**: $w' = w - c$ (interest rate $R = 1$)\n",
    "- **Constraint**: $c \\le w$\n",
    "- **Terminal**: $V_2(w) = \\log(w)$ (consume everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "from lcm import AgeGrid, LinSpacedGrid, Model, Regime, categorical\n",
    "from lcm.typing import BoolND, ContinuousAction, ContinuousState, FloatND, ScalarInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@categorical\n",
    "class RegimeId:\n",
    "    working: int\n",
    "    dead: int\n",
    "\n",
    "\n",
    "def utility(consumption: ContinuousAction) -> FloatND:\n",
    "    return jnp.log(consumption)\n",
    "\n",
    "\n",
    "def terminal_utility(wealth: ContinuousState) -> FloatND:\n",
    "    return jnp.log(wealth)\n",
    "\n",
    "\n",
    "def next_wealth(\n",
    "    wealth: ContinuousState, consumption: ContinuousAction\n",
    ") -> ContinuousState:\n",
    "    return wealth - consumption\n",
    "\n",
    "\n",
    "def next_regime(age: float) -> ScalarInt:\n",
    "    return jnp.where(age >= 1, RegimeId.dead, RegimeId.working)\n",
    "\n",
    "\n",
    "def borrowing_constraint(\n",
    "    consumption: ContinuousAction, wealth: ContinuousState\n",
    ") -> BoolND:\n",
    "    return consumption <= wealth\n",
    "\n",
    "\n",
    "def beta_delta_H(\n",
    "    utility: float,\n",
    "    E_next_V: float,\n",
    "    beta: float,\n",
    "    delta: float,\n",
    ") -> float:\n",
    "    return utility + beta * delta * E_next_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working = Regime(\n",
    "    actions={\n",
    "        \"consumption\": LinSpacedGrid(start=0.001, stop=50, n_points=500),\n",
    "    },\n",
    "    states={\n",
    "        \"wealth\": LinSpacedGrid(\n",
    "            start=0.5, stop=50, n_points=200, transition=next_wealth\n",
    "        ),\n",
    "    },\n",
    "    constraints={\"borrowing_constraint\": borrowing_constraint},\n",
    "    transition=next_regime,\n",
    "    functions={\"utility\": utility, \"H\": beta_delta_H},\n",
    "    active=lambda age: age <= 1,\n",
    ")\n",
    "\n",
    "dead = Regime(\n",
    "    transition=None,\n",
    "    states={\n",
    "        \"wealth\": LinSpacedGrid(start=0.5, stop=50, n_points=200, transition=None),\n",
    "    },\n",
    "    functions={\"utility\": terminal_utility},\n",
    "    active=lambda age: age > 1,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    regimes={\"working\": working, \"dead\": dead},\n",
    "    ages=AgeGrid(start=0, stop=2, step=\"Y\"),\n",
    "    regime_id_class=RegimeId,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom `beta_delta_H` takes `beta` and `delta` as parameters. These appear in the\n",
    "params template under the `H` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(dict(model.params_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution\n",
    "\n",
    "With $\\log$ utility, the optimal consumption rule is $c_t = w_t / D_t$, where $D_t$ is\n",
    "a \"marginal propensity to save\" denominator that depends on the discounting type.\n",
    "\n",
    "**Derivation at $t = 1$** (one period before terminal):\n",
    "\n",
    "$$V_1(w) = \\max_c \\;\\bigl[\\log(c) + \\beta\\delta \\cdot \\log(w - c)\\bigr]$$\n",
    "\n",
    "First-order condition: $1/c = \\beta\\delta / (w - c)$, giving $c_1 = w / (1 + \\beta\\delta)$.\n",
    "\n",
    "**Derivation at $t = 0$** (two periods before terminal):\n",
    "\n",
    "$$V_0(w) = \\max_c \\;\\bigl[\\log(c) + \\beta\\delta \\cdot V_1(w - c)\\bigr]$$\n",
    "\n",
    "Since $V_1(w) = (1 + \\beta\\delta)\\log(w) + \\text{const}$, the FOC gives:\n",
    "\n",
    "$$c_0 = \\frac{w}{1 + \\beta\\delta\\,(1 + \\beta\\delta)}$$\n",
    "\n",
    "Writing $d = \\beta\\delta$ for brevity:\n",
    "\n",
    "| | $D_1$ | $D_0$ |\n",
    "|---|---|---|\n",
    "| **Exponential** ($\\beta = 1$) | $1 + \\delta$ | $1 + \\delta(1 + \\delta)$ |\n",
    "| **Sophisticated** ($\\beta < 1$) | $1 + \\beta\\delta$ | $1 + \\beta\\delta(1 + \\beta\\delta)$ |\n",
    "| **Naive** ($\\beta < 1$) | $1 + \\beta\\delta$ | $1 + \\beta\\delta(1 + \\delta)$ |\n",
    "\n",
    "At $t = 1$, naive and sophisticated are identical — with only one future period, there\n",
    "is no distinction. They differ at $t = 0$: the naive agent uses $V^E$ (solved with\n",
    "$\\delta$), so the inner denominator is $1 + \\delta$ instead of $1 + \\beta\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 0.7\n",
    "DELTA = 0.95\n",
    "W0 = 20.0\n",
    "\n",
    "\n",
    "def analytical_consumption(beta, delta, w0, naive):\n",
    "    \"\"\"Return (c0, c1) from the closed-form solution.\"\"\"\n",
    "    bd = beta * delta\n",
    "    d1 = 1 + bd\n",
    "    if naive:  # noqa: SIM108\n",
    "        # Naive agent thinks future self uses delta, not beta*delta\n",
    "        d0 = 1 + bd * (1 + delta)\n",
    "    else:\n",
    "        d0 = 1 + bd * d1\n",
    "    c0 = w0 / d0\n",
    "    c1 = (w0 - c0) / d1\n",
    "    return c0, c1\n",
    "\n",
    "\n",
    "c0_exp, c1_exp = analytical_consumption(1.0, DELTA, W0, naive=False)\n",
    "c0_soph, c1_soph = analytical_consumption(BETA, DELTA, W0, naive=False)\n",
    "c0_naive, c1_naive = analytical_consumption(BETA, DELTA, W0, naive=True)\n",
    "\n",
    "print(f\"{'Type':<15} {'c_0':>8} {'c_1':>8}\")\n",
    "print(\"-\" * 33)\n",
    "print(f\"{'Exponential':<15} {c0_exp:8.4f} {c1_exp:8.4f}\")\n",
    "print(f\"{'Sophisticated':<15} {c0_soph:8.4f} {c1_soph:8.4f}\")\n",
    "print(f\"{'Naive':<15} {c0_naive:8.4f} {c1_naive:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sophisticated agent consumes more at $t = 0$ than the exponential agent (present\n",
    "bias pulls consumption forward). The naive agent consumes slightly less than the\n",
    "sophisticated agent at $t = 0$ because they (incorrectly) believe their future self\n",
    "will save more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving and Simulating\n",
    "\n",
    "### Exponential Agent ($\\beta = 1$)\n",
    "\n",
    "With $\\beta = 1$, the custom $H$ reduces to the default. We use `solve_and_simulate`\n",
    "with a single params dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_wealth = jnp.array([W0])\n",
    "initial_age = jnp.array([0.0])\n",
    "\n",
    "result_exp = model.solve_and_simulate(\n",
    "    params={\"working\": {\"H\": {\"beta\": 1.0, \"delta\": DELTA}}},\n",
    "    initial_states={\"age\": initial_age, \"wealth\": initial_wealth},\n",
    "    initial_regimes=[\"working\"],\n",
    "    debug_mode=False,\n",
    ")\n",
    "\n",
    "df_exp = result_exp.to_dataframe().query('regime == \"working\"')\n",
    "print(\"Exponential agent:\")\n",
    "print(\n",
    "    f\"  c_0 = {df_exp.loc[df_exp['age'] == 0, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c0_exp:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  c_1 = {df_exp.loc[df_exp['age'] == 1, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c1_exp:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sophisticated Agent ($\\beta < 1$)\n",
    "\n",
    "The sophisticated agent solves and simulates with the same $\\beta\\delta$ parameters —\n",
    "their value function already accounts for future present-bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_soph = model.solve_and_simulate(\n",
    "    params={\"working\": {\"H\": {\"beta\": BETA, \"delta\": DELTA}}},\n",
    "    initial_states={\"age\": initial_age, \"wealth\": initial_wealth},\n",
    "    initial_regimes=[\"working\"],\n",
    "    debug_mode=False,\n",
    ")\n",
    "\n",
    "df_soph = result_soph.to_dataframe().query('regime == \"working\"')\n",
    "print(\"Sophisticated agent:\")\n",
    "print(\n",
    "    f\"  c_0 = {df_soph.loc[df_soph['age'] == 0, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c0_soph:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  c_1 = {df_soph.loc[df_soph['age'] == 1, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c1_soph:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Agent — Manual Approach (Separate Solve/Simulate)\n",
    "\n",
    "The naive agent requires two separate steps:\n",
    "\n",
    "1. **Solve** with $\\beta = 1$ (exponential) to get the perceived continuation values\n",
    "   $V^E$\n",
    "2. **Simulate** with $\\beta < 1$ — actions are chosen using the present-biased\n",
    "   $H(u, V^E) = u + \\beta\\delta \\cdot V^E$, but the continuation values $V^E$ come\n",
    "   from step 1\n",
    "\n",
    "This works because `simulate` uses the pre-computed `V_arr_dict` for continuation\n",
    "values but evaluates $Q$ with its own params for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Solve with exponential discounting (beta=1)\n",
    "V_exponential = model.solve(\n",
    "    params={\"working\": {\"H\": {\"beta\": 1.0, \"delta\": DELTA}}},\n",
    "    debug_mode=False,\n",
    ")\n",
    "\n",
    "# Step 2: Simulate with present-biased action selection\n",
    "result_naive = model.simulate(\n",
    "    params={\"working\": {\"H\": {\"beta\": BETA, \"delta\": DELTA}}},\n",
    "    initial_states={\"age\": initial_age, \"wealth\": initial_wealth},\n",
    "    initial_regimes=[\"working\"],\n",
    "    V_arr_dict=V_exponential,\n",
    "    debug_mode=False,\n",
    ")\n",
    "\n",
    "df_naive = result_naive.to_dataframe().query('regime == \"working\"')\n",
    "print(\"Naive agent:\")\n",
    "print(\n",
    "    f\"  c_0 = {df_naive.loc[df_naive['age'] == 0, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c0_naive:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  c_1 = {df_naive.loc[df_naive['age'] == 1, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c1_naive:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Agent — `PhaseVariant` Approach\n",
    "\n",
    "`PhaseVariant` lets you provide different function implementations for the solve and\n",
    "simulate phases in a single model. The solve phase uses exponential discounting for\n",
    "backward induction while the simulate phase applies present bias for action selection.\n",
    "\n",
    "The key advantage: you call `solve_and_simulate` once with one params dict instead of\n",
    "manually managing separate solve/simulate steps.\n",
    "\n",
    "Each variant can have a **different signature**. The params template is the union of\n",
    "both variants' parameters; each variant receives only the kwargs it expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm import PhaseVariant\n",
    "\n",
    "\n",
    "def exponential_H(\n",
    "    utility: float,\n",
    "    E_next_V: float,\n",
    "    discount_factor: float,\n",
    ") -> float:\n",
    "    return utility + discount_factor * E_next_V\n",
    "\n",
    "\n",
    "# Build a model where H uses exponential discounting for solve,\n",
    "# but beta-delta discounting for simulate.\n",
    "working_pv = Regime(\n",
    "    actions={\n",
    "        \"consumption\": LinSpacedGrid(start=0.001, stop=50, n_points=500),\n",
    "    },\n",
    "    states={\n",
    "        \"wealth\": LinSpacedGrid(\n",
    "            start=0.5, stop=50, n_points=200, transition=next_wealth\n",
    "        ),\n",
    "    },\n",
    "    constraints={\"borrowing_constraint\": borrowing_constraint},\n",
    "    transition=next_regime,\n",
    "    functions={\n",
    "        \"utility\": utility,\n",
    "        \"H\": PhaseVariant(\n",
    "            solve=exponential_H,  # V^E for backward induction\n",
    "            simulate=beta_delta_H,  # present-biased action selection\n",
    "        ),\n",
    "    },\n",
    "    active=lambda age: age <= 1,\n",
    ")\n",
    "\n",
    "model_pv = Model(\n",
    "    regimes={\"working\": working_pv, \"dead\": dead},\n",
    "    ages=AgeGrid(start=0, stop=2, step=\"Y\"),\n",
    "    regime_id_class=RegimeId,\n",
    ")\n",
    "\n",
    "# Params are the UNION of both variants' parameters.\n",
    "# exponential_H needs discount_factor; beta_delta_H needs beta and delta.\n",
    "result_naive_pv = model_pv.solve_and_simulate(\n",
    "    params={\n",
    "        \"working\": {\n",
    "            \"H\": {\"discount_factor\": DELTA, \"beta\": BETA, \"delta\": DELTA},\n",
    "        },\n",
    "    },\n",
    "    initial_states={\"age\": initial_age, \"wealth\": initial_wealth},\n",
    "    initial_regimes=[\"working\"],\n",
    "    debug_mode=False,\n",
    ")\n",
    "\n",
    "df_naive_pv = result_naive_pv.to_dataframe().query('regime == \"working\"')\n",
    "print(\"Naive agent (PhaseVariant):\")\n",
    "print(\n",
    "    f\"  c_0 = {df_naive_pv.loc[df_naive_pv['age'] == 0, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c0_naive:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  c_1 = {df_naive_pv.loc[df_naive_pv['age'] == 1, 'consumption'].iloc[0]:.4f}  \"\n",
    "    f\"(analytical: {c1_naive:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let's compare all three agent types. The small differences between numerical and\n",
    "analytical solutions are due to grid discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Agent\": [\"Exponential\", \"Sophisticated\", \"Naive\"],\n",
    "        \"c_0 (numerical)\": [\n",
    "            df_exp.loc[df_exp[\"age\"] == 0, \"consumption\"].iloc[0],\n",
    "            df_soph.loc[df_soph[\"age\"] == 0, \"consumption\"].iloc[0],\n",
    "            df_naive.loc[df_naive[\"age\"] == 0, \"consumption\"].iloc[0],\n",
    "        ],\n",
    "        \"c_0 (analytical)\": [c0_exp, c0_soph, c0_naive],\n",
    "        \"c_1 (numerical)\": [\n",
    "            df_exp.loc[df_exp[\"age\"] == 1, \"consumption\"].iloc[0],\n",
    "            df_soph.loc[df_soph[\"age\"] == 1, \"consumption\"].iloc[0],\n",
    "            df_naive.loc[df_naive[\"age\"] == 1, \"consumption\"].iloc[0],\n",
    "        ],\n",
    "        \"c_1 (analytical)\": [c1_exp, c1_soph, c1_naive],\n",
    "    }\n",
    ")\n",
    "comparison = comparison.set_index(\"Agent\")\n",
    "comparison.style.format(\"{:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Beta-delta discounting in PyLCM requires no core modifications. The key ingredients\n",
    "are:\n",
    "\n",
    "1. **Custom $H$ function** with `beta` and `delta` parameters:\n",
    "   ```python\n",
    "   def beta_delta_H(utility, E_next_V, beta, delta):\n",
    "       return utility + beta * delta * E_next_V\n",
    "   ```\n",
    "\n",
    "2. **Pass it to the regime** via `functions={\"utility\": ..., \"H\": beta_delta_H}`\n",
    "\n",
    "3. **Set parameters** via the params dict under the `\"H\"` key:\n",
    "   ```python\n",
    "   params = {\"working\": {\"H\": {\"beta\": 0.7, \"delta\": 0.95}}}\n",
    "   ```\n",
    "\n",
    "4. **For naive agents**, there are two approaches:\n",
    "\n",
    "   *Manual*: call `solve` and `simulate` separately with different params.\n",
    "\n",
    "   *`PhaseVariant`*: wrap $H$ so the solve phase uses exponential discounting\n",
    "   while the simulate phase applies present bias, then call `solve_and_simulate`\n",
    "   as usual:\n",
    "   ```python\n",
    "   from lcm import PhaseVariant\n",
    "\n",
    "   functions={\n",
    "       \"utility\": utility,\n",
    "       \"H\": PhaseVariant(\n",
    "           solve=exponential_H,    # V^E for backward induction\n",
    "           simulate=beta_delta_H,  # present-biased action selection\n",
    "       ),\n",
    "   }\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
